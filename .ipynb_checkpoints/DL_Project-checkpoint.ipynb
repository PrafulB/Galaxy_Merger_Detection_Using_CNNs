{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-64fbac93e84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ssh' is not defined"
     ]
    }
   ],
   "source": [
    "ssh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_array(index, length):\n",
    "\n",
    "    one_hot = np.zeros(length)\n",
    "    one_hot[index] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def make_stratified_generator(class_paths, target_size=(299, 299), individual_batchsize=8, zoom_factor=2., translation_range=0.05):\n",
    "\n",
    "    zoom_base = math.sqrt(2) * ((1 - translation_range) ** -1)\n",
    "\n",
    "    generators = []\n",
    "    for class_path in class_paths:\n",
    "\n",
    "        image_data_generator = image.ImageDataGenerator(rotation_range=360,\n",
    "                                                        zoom_range=(zoom_base, zoom_base * zoom_factor),\n",
    "                                                        width_shift_range=translation_range,\n",
    "                                                        height_shift_range=translation_range,\n",
    "                                                        vertical_flip=True)\n",
    "\n",
    "        generator = image_data_generator.flow_from_directory(class_path,\n",
    "                                                             batch_size=individual_batchsize,\n",
    "                                                             target_size=target_size)\n",
    "        generators += [generator]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        images = []\n",
    "        for generator in generators:\n",
    "            images += [next(generator)][0]\n",
    "\n",
    "        cumulative_batch = np.vstack(images)\n",
    "\n",
    "        cumulative_labels = np.vstack(\n",
    "                [np.tile(one_hot_array(i, len(class_paths)), (len(imgs), 1)) for i, imgs in enumerate(images)]\n",
    "                )\n",
    "\n",
    "        yield cumulative_batch, cumulative_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --path_images PATH_IMAGES --path_checkpoints\n",
      "                             PATH_CHECKPOINTS --path_statuses PATH_STATUSES\n",
      "                             --mode MODE\n",
      "ipykernel_launcher.py: error: the following arguments are required: --path_images, --path_checkpoints, --path_statuses, --mode\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pap408/pyenv/py3.5.3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def write_status(filename, epoch_nr, training_error, validation_error, training_loss, validation_loss):\n",
    "\n",
    "    info = {'epoch_nr': epoch_nr,\n",
    "            'timestamp': time.time(),\n",
    "            'training_error': training_error,\n",
    "            'validation_error': validation_error,\n",
    "            'training_loss': training_loss,\n",
    "            'validation_loss': validation_loss}\n",
    "\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(info, fp)\n",
    "\n",
    "\n",
    "def make_generators(path_images, positive_class='merger', negative_class='noninteracting'):\n",
    "\n",
    "    training_gen = make_stratified_generator(Path(path_images) / positive_class / 'training',\n",
    "                                             Path(path_images) / negative_class / 'training')\n",
    "\n",
    "    validation_gen = make_stratified_generator(Path(path_images) / positive_class / 'validation',\n",
    "                                               Path(path_images) / negative_class / 'validation')\n",
    "\n",
    "    return training_gen, validation_gen\n",
    "\n",
    "\n",
    "def training_loop(model, nr_epochs, path_images, training_type, steps_per_epoch = (6000 * 2/4), validation_steps = (6000 * 1/4)):\n",
    "\n",
    "    training_gen, validation_gen=make_generators(path_images)\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(nr_epochs), total = nr_epochs):\n",
    "\n",
    "        logging.info(\"starting {training_type} epoch #{epoch}\".format(training_type, epoch))\n",
    "        history=model.fit_generator(training_gen, steps_per_epoch = steps_per_epoch, epochs = 1,\n",
    "                                    validation_data = validation_gen, validation_steps = validation_steps)\n",
    "        logging.info(\"finished {training_type} epoch #{epoch}\".format(training_type, epoch))\n",
    "\n",
    "        model.save(Path(path_checkpoints) / '{training_type}_{epoch}.checkpoint'.format(training_type, epoch))\n",
    "\n",
    "        training_error, validation_error = (1. - history.history['acc'][0]), (1. - history.history['val_acc'][0])\n",
    "        training_loss, validation_loss = history.history['loss'][0], history.history['val_loss'][0]\n",
    "\n",
    "        write_status(Path(path_statuses) / '{training_type}_{epoch}.status'.format(training_type, epoch),\n",
    "                     epoch,\n",
    "                     training_error,\n",
    "                     validation_error,\n",
    "                     training_loss,\n",
    "                     validation_loss)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def preparation_training(path_images, path_checkpoints, path_statuses, nr_epochs = 40):\n",
    "\n",
    "    if mode == 'transferlearning':\n",
    "        xception=Xception(weights = 'imagenet', include_top = False)\n",
    "    else:\n",
    "        xception=Xception(weights = None, include_top = False)\n",
    "\n",
    "    output=GlobalAveragePooling2D()(xception.output)\n",
    "    output=Dense(1024, activation = 'relu')(output)\n",
    "    output=Dense(2, activation = 'softmax')(output)\n",
    "    model=Model(input = xception.input, output = output)\n",
    "\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable=False  # freeze all layers except the new FC layers at the end\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model = training_loop(model, nr_epochs, path_images, training_type='preparation')\n",
    "\n",
    "    model.save(Path(path_checkpoints) / 'main_0.checkpoint')\n",
    "\n",
    "\n",
    "def main_training(path_images, path_checkpoints, path_statuses, current_epoch, nr_epochs=100000):\n",
    "\n",
    "    current_checkpoint_filename = Path(path_checkpoints) / 'main_{current_epoch}.checkpoint'.format(current_epoch)\n",
    "    model = load_model(current_checkpoint_filename)\n",
    "\n",
    "    for layer in model.layers:\n",
    "       layer.trainable = True # unfreeze all layers\n",
    "\n",
    "    model.compile(optimizer=SGD(lr=0.5 * 3e-5, momentum=0.9), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    training_loop(model, nr_epochs, path_images, training_type='main')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--path_images', required=True)\n",
    "    parser.add_argument('--path_checkpoints', required=True)\n",
    "    parser.add_argument('--path_statuses', required=True)\n",
    "    parser.add_argument('--mode', required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    path_images, path_checkpoints, path_statuses, mode = (args.path_images,\n",
    "                                                          args.path_checkpoints,\n",
    "                                                          args.path_statuses,\n",
    "                                                          args.mode)\n",
    "\n",
    "    logging.basicConfig(filename='LOG.log', level=logging.DEBUG)\n",
    "\n",
    "\n",
    "    logging.info((\"started with parameters\",\n",
    "                 \"path_images: '{path_images}''\".format(path_images),\n",
    "                 \"path_checkpoints: '{path_checkpoints}''\".format(path_checkpoints),\n",
    "                 \"mode: '{mode}''\".format(mode)))\n",
    "\n",
    "\n",
    "    logging.info(\"starting new training\")\n",
    "    preparation_training(path_images, path_checkpoints, path_statuses, mode)\n",
    "\n",
    "    logging.info(\"starting main training\")\n",
    "    main_training(path_images, path_checkpoints, path_statuses, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
